{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"http://www.digitalconversations.com.au/wp-content/uploads/2017/08/boost.jpg\" height=\"300\">\n",
    "<br></br> \n",
    "<br></br> \n",
    "<br></br> \n",
    "\n",
    "<h1><center>Boosting Models</center></h1>\n",
    "<div class=\"pull-right\"><h3>Ybigta Design Team 11기 김도영</h3></div>\n",
    "<br></br> \n",
    "<br></br> \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<h2>Intro</h2>\n",
    "\n",
    "Boosting Model에 대해 개인적으로 공부한 내용을 정리한 글입니다.<br></br> \n",
    "다양한 글을 편집하여 구성했으며, 출처는 글 마지막에 정리해두겠습니다.<br></br> \n",
    "\n",
    "요즘 가장 핫한 XGBOOST를 이해하기 위해서는 먼저 Boosting이 무엇인지 알아야 합니다.<br></br> \n",
    "<br></br> \n",
    "<br></br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>모형 결합의 기초</h2><br></br>\n",
    "<br></br> \n",
    "Boosting을 알아보기에 앞서, 이의 상위개념인 모형 결합에 대해 잠시 짚고 넘어가겠습니다.<br></br>\n",
    "어디까지나 Boosting의 이해를 돕기 위한 내용이므로, 자세한 부분이 이해되지 않는다면 그냥 넘어가셔도 좋습니다.<br></br>\n",
    "\n",
    "모형 결합(model combining) 방법은 앙상블 방법론(ensemble methods)이라고도 합니다.<br></br> \n",
    "이는 복수의 예측 모형을 결합하여 더 나은 예측 결과를 얻기 위한 시도로 볼 수 있습니다.<br></br> \n",
    "<br></br> \n",
    "모형 결합 방법을 사용하면 일반적으로 계산량은 증가하지만,<br></br> \n",
    "여러 모형의 (가중)평균치를 예측치로 사용하므로 variance가 감소하여 overfitting을 방지할 수 있습니다.<br></br> \n",
    "(variance - bias tradeoff에 대해 잘 모르거나 더 알고싶다면 <a href=\"http://sohnnn.tistory.com/entry/Bias-and-Variance-tradeoff-1\">여기</a>를 참고하세요)<br></br> \n",
    "(bias는 일정 수준으로 유지하면서 variance는 줄이는 bagging에 대한 자세한 설명은 <a href=\"https://www.quora.com/What-does-Bagging-reduces-the-variance-while-retaining-the-bias-mean\">여기</a>에 있어요!)<br></br> \n",
    "<br></br> \n",
    "<br></br> \n",
    "\n",
    "모형 결합 방법은 크게 취합(aggregation) 방법론과 부스팅(boosting) 방법론으로 나뉩니다.<br></br> \n",
    "<br></br> \n",
    "\n",
    "취합 방법론은 사용할 모형의 집합이 '이미' 결정되어 있지만, 부스팅 방법론은 사용할 모형을 '점진적으로' 늘려나갑니다.<br></br>\n",
    "(취합 방법론은 모델의 병렬결합, 부스팅 방법론은 모델의 연속적결합으로 이해해주세요.)<br></br>\n",
    "<a href=\"https://www.youtube.com/watch?v=2Mg8QD0F1dQ\">3분만에 Bagging 이해하기</a><br></br>\n",
    "<a href=\"https://www.youtube.com/watch?v=GM3CDQfQ4sw\">3분만에 Boosting 이해하기</a><br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "<img src=\"https://quantdare.com/wp-content/uploads/2016/04/bb3-800x307.png\">\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "* 취합 방법론이 적용된 대표 알고리즘\n",
    "\n",
    "> - 다수결 (Majority Voting)\n",
    "> - 배깅 (Bagging)\n",
    "> - 랜덤 포레스트 (Random Forests)\n",
    " \n",
    "\n",
    "* 부스팅 방법론이 적용된 대표 알고리즘\n",
    "\n",
    "> - [애-다]부스트 (AdaBoost)\n",
    "> - 그레디언트 부스트 (Gradient Boost)\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Boosting이란?</h2>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "Boosting이란 무작위로 선택하는 것보다 약간 가능성이 높은 규칙(이것을 전문 용어로는 weak learner/classifier라고 합니다.)들을 결합시켜 보다 정확한 예측 모델을 만들어 내는 것을 말합니다. Boosting이란 이름이 붙은 이유도 약한 것들을 여러 개 결합시켜 강한 모델을 만들어낸다는 의미에서 붙은 것이지요.\n",
    "\n",
    "<br></br>\n",
    "Boosting의 역사는 한참을 거슬러 올라가지만, 1989년 Schapire가 제대로 된 모델을 발표했으며, 그 이후 Freund가 좀 더 효율적인 Boosting 알고리즘을 발표하게 됩니다. Boosting은 주로 classification에 관련된 문제를 해결하기 위해 고안이 되었지만, 통계 등에서 사용하는 regression에도 사용할 수 있는 훌륭한 알고리즘이며, 일부 사람들은 90년대에 개발된 최고의 머신러닝 알고리즘 중 하나라고 평하기도 합니다.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "이해를 돕기 위해 가장 많이 쓰이는 예시를 하나 가져왔습니다.\n",
    "\n",
    "가령 내가 받은 메일이 스팸메일인지 아닌지의 여부를 가릴 수 있는 방법을 개발한다고 가정합시다.<br></br> \n",
    "스팸 여부를 판정할 수 있는 기준은 많으며, 예를 들어 아래과 같은 것들이 있습니다. (간단한 규칙들에 대하여 Yes/No로만 판정!)<br></br>\n",
    "\n",
    "\n",
    "- 링크만 있는 경우 => 스팸\n",
    "- 메일 내용에 “당신의 보험료가 xxx 입니다”라는 내용이 들어 있는 경우 => 스팸\n",
    "- 도메인 주소가 확실한 경우 => 스팸 아님\n",
    "- 보낸 사람이 확실한 경우 => 스팸 아님\n",
    "\n",
    "<img src=\"https://1335865630.rsc.cdn77.org/images/Spam-filter.jpg\">\n",
    "\n",
    "스팸 여부를 판단할 수 있는 기준들은 위에서 열거한 일부 경우 외에도 아주 많습니다. <br></br>\n",
    "그런데 위에서 열거한 기준들은 모든 메일에 똑같이 적용할 수 있을 정도로 확실하거나 강력한 규칙은 아닙니다. <br></br>\n",
    "바로 이런 것들을 weak learner(rule, classifier)라고 합니다.<br></br>\n",
    "\n",
    "<br></br>\n",
    "\n",
    "그럼 이러한 weak learner를 strong learner로 바꿀 수 있는 방법은 무엇일까요?<br></br> \n",
    "아무래도 여러 weak learner들을 **하나로 합친다면**, 가능하지 않을까요?<br></br> \n",
    "\n",
    "여러 weak learner들을 **하나로 합치는** 방법에는 \n",
    "\n",
    "- 평균/ 가중 평균(weighted average)를 사용하는 방법\n",
    "- 가장 많은 의견(vote)을 얻은 것을 선정하는 방법\n",
    "\n",
    "과 같은 방법을 생각해 볼 수 있겠네요.\n",
    "\n",
    "앞서 살펴본 스팸 메일 여부를 가리는 규칙에 위 두가지 방법을 적용하면, 좀 더 강력한 스팸 필터가 만들어질 것이며, \n",
    "규칙이 더 늘어나면 늘어날수록 더 좋은 결과를 기대할 수 있게 될 것입니다.\n",
    "<br></br>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Boosting 학습 방법</h2>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "우리는 이제 weak learner(a.k.a. base learner)들이 모여 하나의 strong rule을 만든다는 사실을 알게 되었습니다.<br></br>\n",
    "그렇다면 먼저 그 'weak learner'들이 어떻게 결정되는지 살펴보아야겠죠.<br></br>\n",
    "\n",
    "Boosting의 가장 기본적인 아이디어는 \"이전 모델이 많이 틀리는 부분에 집중한다!\" 입니다.<br></br>\n",
    "\n",
    "일단 가장 먼저 학습하는 첫번째 weak learner는 아무런 사전정보가 없으므로, 가중치가 전혀 부여되지 않은 데이터셋을 바탕으로 학습합니다.<br></br>\n",
    "그 결과로 예측한 값에는 어느 정도 오차가 있을 수밖에 없겠죠.<br></br>\n",
    "\n",
    "그리고 여기서부터 중요한 내용이 등장합니다.<br></br>\n",
    "그 다음 모델은 그 오차가 발생한 데이터, 즉 맞추지 못한 데이터에 가중치가 부여된 데이터셋에서 resampling된 데이터로 학습을 하게됩니다.<br></br>\n",
    "가중치가 부여된 데이터는 다른 데이터에 비해 추출될 확률이 더 높습니다.<br></br>\n",
    "(기본적으로 기존 데이터를 training set과 test set으로 나누고, training set 안에서 비복원 추출, 가중치가 부여되면 여러번 뽑힐 확률 증가)<br></br>\n",
    "이렇게하면 두번째 모델은 아무래도 첫번째 모델이 틀렸던 부분에 더 특화된 모델이 되지 않을까요?<br></br>\n",
    "\n",
    "이 과정에서 이전보다 더 높은 예측률이 관찰되면 이 두번째 모델은 두번째 weak learner로 선정되고, <br></br>\n",
    "첫번째 모델과 두번째 모델 모두 잘 맞추지 못한 데이터에는 다시 가중치가 부여되어 세번째 모델의 데이터셋에 영향을 주게 됩니다.<br></br>\n",
    "\n",
    "<img src=\"https://littleml.files.wordpress.com/2017/03/boosted-trees-process.png\"><br></br>\n",
    "\n",
    "이와 같은 과정을 거치면서 K개의 weak learner가 생성되고,<br></br>\n",
    "하나의 strong rule은 각 weak learner들의 가중평균으로 구성됩니다. <br></br>\n",
    "그리고 각 weak learner들에 부여되는 가중치는 해당 모델의 예측력을 기준으로 합니다. <br></br>\n",
    "예측력이 높다면 해당 모델은 좀 더 높은 가중치를 부여받겠죠. <br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "Bagging에 대해 어느 정도 이해하셨다면, Boosting은 Bagging에서 bias까지 컨트롤하기위해 좀 더 발전한 형태라는 점을 눈치채셨을겁니다.<br></br>\n",
    "아무래도 이전 모델이 잘 맞추지 못하는 영역에 집중하면 당연히 bias는 줄어들겠죠.<br></br>\n",
    "이런 점 때문에 Boosting이 마치 variance와 bias를 모두 컨트롤하는 것처럼 느껴지기도 합니다.<br></br>\n",
    "\n",
    "물론 Boosting을 통해 variance와 bias를 둘 다 낮출 수 있는 것은 사실입니다. (https://www.quora.com/What-effect-does-boosting-have-on-bias-and-variance)<br></br>\n",
    "위에서도 언급했지만 기본적으로 앙상블 기법은 variance를 낮춰 좀 더 일반적인(generalized) 모델을 만들어줍니다.<br></br>\n",
    "하지만 이전 모델들이 잘 맞추지 못하는 포인트에 지속적으로 집중하여 weak learner를 만들어내다보면,<br></br> \n",
    "결국 strong rule 자체는 training set에 과적합될 수밖에 없습니다.<br></br>\n",
    "항상 overfitting의 위험성을 가지고 있는 것이죠. <br></br>\n",
    "\n",
    "뒤에서 설명할 Adaptive boosting은 쉽게 overfitting되지 않지만, Gradient boosting의 경우 과적합의 위험을 항상 가지고 있으므로 좋은 성능을 내기 위해서는 파라미터 튜닝이 꼭 필요합니다. (https://www.quora.com/Why-is-the-boosting-algorithm-robust-to-overfitting)<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "어쩌다보니 단점부터 소개하게 되었지만, Boosting 모델에는 사실 장점이 훨씬 더 많습니다.<br></br>\n",
    "실제 Kaggle Competition에서 우승한 사람들 대부분은 GradientBoost를 개량한 XGBoost를 사용하였고, 이는 Boosting 모델이 그만큼 훌륭한 범용성을 지니고 있다는 증거일 것입니다.<br></br>\n",
    "XGBoost를 개발한 Tianqi Chen 또한 이 모델이 Classification, regression, ranking 등 대부분의 문제에 적용될 수 있다고 말하고 있습니다.<br></br> \n",
    "그야말로 믿고 쓰는 XGBoost가 아닐 수 없습니다.<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Boosting Model 소개</h2>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "| 알고리즘 | 특징 | \n",
    "| ---\n",
    "| AdaBoost | 다수결을 통한 정답 분류 및 오답 가중치 부여 | \n",
    "| GradientBoost | Loss Function의 gradient를 통해 오답에 가중치 부여  | \n",
    "| XGBoost |  GradientBoost를 효율적으로 개선<br></br> 시스템 자원 효율적 활용 ( CPU, Mem)<br></br> 2014년 공개 | \n",
    "| Light GBM | Xgboost 대비 성능향상 및 자원소모 최소화<br></br> Xgboost가 처리하지 못하는 대용량 데이터 학습 가능<br></br> Approximates the split (근사치의 분할)을 통한 성능 향상<br></br> 2016년 공개  |\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "아래에서 다룰 알고리즘은 AdaBoost, GradientBoost, XGBoost로 총 세가지이지만, XGBoost의 원리 자체는 GradientBoost와 동일하므로 자세한 설명은 \n",
    "AdaBoost와 GradientBoost에 치중할 예정입니다.\n",
    "Light GBM은 가장 최근에 공개되었고, 그만큼 사용 가능한 툴도 적을 뿐더러 저 또한 사용해보지 못했으므로 다음 기회에 알아보도록 합시다.<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/javascript\"  src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"></script> \n",
    "\n",
    "<h2>Adaptive Boosting (AdaBoost)</h2>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "AdaBoost는 앞서 살펴본 부스팅 알고리즘을 좀 더 발전시킨 개념이라고 볼 수 있습니다. <br></br>\n",
    "기존 앙상블 알고리즘의 경우 각 개별 분류기의 “committee”에서 의사결정 가중치가 모두 동일하게 이뤄졌다면, <br></br>\n",
    "AdaBoost는 개별 Classifier의 의사결정 가중치를 서로 달리하는 개념이 추가적으로 반영된 알고리즘이라고 할 수 있습니다.<br></br>\n",
    "쉽게 말해 개별 weak learner에 가중치를 적용한다고 이해하시면 됩니다.<br></br>\n",
    "<br></br>\n",
    "<img src=\"http://postfiles10.naver.net/MjAxNzAxMTVfNTAg/MDAxNDg0NDgyNDQ4NTQ4.aBH0a8HLOkwPoHEph3WeCsmlhXlWXsY6qlmztEVFU50g.H7rwuKVxP_xp8meQt5uJOyoqp10p8ukRc9GuEffWMKog.PNG.gksshdk8003/%EA%B7%B8%EB%A6%BC1.png?type=w773\">\n",
    "<br></br>\n",
    "\n",
    "알고리즘의 진행순서는 위와 같습니다.\n",
    "\n",
    "1. 먼저 모든 데이터 값의 가중치를 동일하게 설정합니다.\n",
    "2. 전체 M개의 weak learner를 가정했을 때, 아래의 과정을 순차적으로 실행합니다. \n",
    " - (a) training data 각각에는 현재 \\\\(w_i \\\\)의 개별 가중치가 부여되어 있습니다. 여기에서 bootstrapping한 data set으로 classifier(weak learner)를 학습시킵니다.\n",
    " - (b) 위의 식과 같이 error를 계산합니다. I function은 indicator funtion으로 괄호 안의 값이 True이면 1을, 그렇지 않을 경우 0을 반환합니다. m번째 모델의 error값은 0과 1사이의 값을 가지겠네요\n",
    " - (c) 계산한 error값을 사용하여 m번째 모델의 가중치를 계산, 갱신합니다. 식을 잘 살펴보면 error값이 커질수록 가중치인 \\\\(\\alpha \\\\) 는 작아지는 것을 확인할 수 있습니다.\n",
    " - (d) 개별 데이터의 가중치 \\\\(w_i \\\\)를 갱신합니다. exp funtion은 \\\\( e^x \\\\), 즉 e의 지수승을 나타내는 함수입니다. 지수가 되는 값은 m번째 모델의 가중치 \\\\(\\alpha \\\\)에 indicator funtion값을 곱한 값입니다. 만약 m번째 모델이 해당 데이터를 잘 맞췄다면 \\\\( e^0 \\\\), 즉 1이 되어 가중치에는 변화가 없습니다. 하지만 틀렸다면 \\\\( e^\\alpha \\\\) > 1만큼의 가중치를 받게되겠죠.<br></br>\n",
    " 이런 식으로 매 라운드마다 지속적으로 개별 모델의 가중치 \\\\(\\alpha \\\\)와 개별 데이터의 가중치 \\\\(w_i \\\\)가 갱신되는 것을 확인할 수 있습니다.\n",
    "3. 최종 아웃풋인 G(x), 즉 \\\\(y_i \\\\)의 예측치는 가중치가 부여된 개별 모델들의 예측값의 summation을 통해 산출됩니다. sign function은 괄호 안의 값의 부호를 판별하는 함수로, -1,0,1의 value를 갖습니다. 대부분의 값은 -1 또는 1로 분류될테니 Binary classification으로 이해하면 되겠군요.\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "좀 더 쉬운 이해를 위해 수식이 아닌 그림을 참고해보도록 하겠습니다.<br></br>\n",
    "사실 위에서 설명한 Boosting과 큰 차이점이 없습니다.<br></br>\n",
    "\n",
    "<img src=\"https://www.packtpub.com/graphics/9781788295758/graphics/image_04_046-1.png\" width=\"\">\n",
    "\n",
    "첫번째 이터레이션에서 +와 -를 분류한 결과, -로 예측한 부분에 3개의 +가 섞여있습니다.(error)<br></br> \n",
    "따라서 -로 잘못 분류된 +들은 가중치를 받고,(두번째 이터레이션에서 크기가 커졌죠?) 그에 따라 두번째 모델은 +와 -를 분류합니다.<br></br> \n",
    "이러한 과정이 반복되고, 가중치가 부여된 모델들의 앙상블로 좋은 분류기를 만들어내는 과정을 확인할 수 있습니다.<br></br> \n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>AdaBoost practice</h2>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "그럼 실제 데이터에 Adaboost를 한번 적용해볼텐데요, 그에 앞서 데이터 전처리 문제가 남아있습니다.<br></br>\n",
    "AdaBoost를 데이터에 fitting시키기 전에 주의해야할 사항에는 어떤 것들이 있을까요?<br></br>\n",
    "\n",
    "- 사실 tree 기반 모델은 전처리할 일이 별로 없습니다. 스케일링? 정규화? 다중공선성? 고려하지 않아도 좋습니다. 여기서 Adaboost를 tree 기반 모델이라고 하는 이유는 weak learner로 보통 CART(Classification and Regression Trees, Breiman et al.(1984))가 쓰이기 때문입니다.\n",
    "- 결측치 또한 신경쓰지 않아도 됩니다. Adaboost에서 결측치는 무시됩니다.\n",
    "- 중요한 것은 Outliers, 즉 특이값입니다. 위의 수식을 주의깊게 보셨다면, 개별 데이터의 가중치를 결정할때 exponential loss function이 사용된 것을 확인할 수 있을 것입니다. 그럼, 생각해봅시다. 1과 10의 차이는 고작 9이지만, e의 1승과 e의 9승의 차이는 어떨까요? classification의 문제야 별 상관이 없다 치더라도, regression의 경우 가중치를 계산할 때 엄청난 차이가 발생할 것입니다. 그리고 그 엄청난 차이는 보통 Outliers에서 발생합니다. regression 문제에 Adaboost를 사용하려 한다면 꼭 특이값을 처리한 후 fitting시킵시다. \n",
    "\n",
    "결과적으로 우린 일단은 특이값에만 신경써서 전처리하면 되겠군요.<br></br>\n",
    "<br></br>\n",
    "한가지 더, Feature Selection 과정에서 더미화한 Categorical feature들은 굳이 하나 빼지 말고 모두 적용해야합니다.<br></br>\n",
    "회귀모델에서야 다중공선성 제거를 위해 한다고하지만, tree 기반 모델에서 value를 하나 지우면 이는 정보의 손실입니다. <br></br>\n",
    "<br></br>\n",
    "이제 데이터에 Adaboost를 한번 적용해봅시다.<br></br>\n",
    "마침 사이킷런에 좋은 예제가 있어 가져와보았습니다.<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25e02e525c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "# Author: Noel Dawe <noel.dawe@gmail.com>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# importing necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "# Create the dataset\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.linspace(0, 6, 100)[:, np.newaxis]\n",
    "y = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])\n",
    "\n",
    "# Fit regression model\n",
    "regr_1 = DecisionTreeRegressor(max_depth=4)\n",
    "\n",
    "regr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),\n",
    "                          n_estimators=300, random_state=rng)\n",
    "\n",
    "regr_1.fit(X, y)\n",
    "regr_2.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_1 = regr_1.predict(X)\n",
    "y_2 = regr_2.predict(X)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure()\n",
    "plt.scatter(X, y, c=\"k\", label=\"training samples\")\n",
    "plt.plot(X, y_1, c=\"g\", label=\"n_estimators=1\", linewidth=2)\n",
    "plt.plot(X, y_2, c=\"r\", label=\"n_estimators=300\", linewidth=2)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Boosted Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "무작위로 생성된 데이터에 1개의 weak learner와 300개의 weak learner를 결합한 AdaboostRegressor를 동시에 학습시키고 그 결과를 비교한 것입니다.<br></br> tree 갯수가 300개 정도인데도 굉장히 잘 예측하는 것을 확인할 수 있는데요.\n",
    "\n",
    "\n",
    "여기에서 AdaBoost의 또다른 장점 하나를 더 확인할 수 있습니다.\n",
    "바로 tunning할 parameter가 아주 적다는 점이죠.\n",
    "\n",
    "tab키를 눌러 parameter를 확인해보면 놀랍게도 고작 5개 뿐입니다. \n",
    "\n",
    "AdaBoostRegressor(base_estimator=None, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None)\n",
    "\n",
    "- base_estimator는 지정하지 않을 경우 디폴트 값으로 의사결정나무를 사용합니다\n",
    "- n_estimators는 사용할 weak learner의 갯수입니다. 보통 많은 learner를 사용할수록 모델의 퍼포먼스도 향상됩니다.\n",
    "- learning_rate는 말 그대로 학습 속도입니다. n_estimators와는 trade-off 관계에 있습니다. learning_rate가 작아서 조금씩 학습할수록 n_estimators는 더 많이 필요하겠죠. 대신 이럴 경우 더 정교한 학습이 가능해져 모델의 퍼포먼스가 향상됩니다. 보통 learning_rate는 작으면 작을수록 좋다고 하네요. 0.1 정도에서 시작하여 0.01 정도의 수준에서 최종 튜닝을 하라고 합니다. <a href=\"https://stats.stackexchange.com/questions/303998/tuning-adaboost\">여기서요...</a>\n",
    "- loss는 loss funtion에 관한 parameter입니다. linear로 설정되었으니 exp로 설정했을때 우리가 우려하는 특이값의 영향에서 자유로울 수 있겠네요!\n",
    "- random_state는 resampling할때 사용하는 seed에 관련된 parameter인듯 합니다. 중요해보이진 않네요.\n",
    "<br></br>\n",
    "<br></br>\n",
    "적정한 parameter의 값은 grid search via cross validation를 적용하면 찾을 수 있습니다.<br></br>\n",
    "생성된 데이터를 가지고 grid search까지 해보지는 않겠습니다. <br></br>\n",
    "잘 아시겠지만 적정 parameter 값을 찾는 일은 단순 노가다에 가깝기 때문이죠.<br></br>\n",
    "사이킷런 라이브러리를 이용하면 그래도 쉽게 구현할 수 있으니 참고하시기 바랍니다. (http://cyan91.tistory.com/18)<br></br>\n",
    "해당 예제와 AdaboostRegressor에 대한 더 자세한 설명은 사이킷런 페이지를 참고하세요. \n",
    "(http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html)\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/javascript\"  src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"></script> \n",
    "\n",
    "\n",
    "<h2>Gradient boosting (GradientBoost)</h2>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "\n",
    "Gradient boosting 또한 위에서 설명한 boosting의 기본 원리에 크게 벗어나는 내용은 없습니다. <br></br>\n",
    "다만 adaptive boosting과 다른 점은 손실함수(loss function)에 Gradient descent 기법을 적용했다는 것뿐입니다. <br></br>\n",
    "\n",
    "그렇다면 먼저 Gradient descent algorithm에 대해 간단히 알아보겠습니다.<br></br>\n",
    "아래 내용은 해당 블로그의 내용을 옮긴 것입니다 (+ 개인적인 추가 설명) (https://medium.com/@peteryun/ml-%EB%AA%A8%EB%91%90%EB%A5%BC-%EC%9C%84%ED%95%9C-tensorflow-3-gradient-descent-algorithm-%EA%B8%B0%EB%B3%B8-c0688208fc59)<br></br>\n",
    "\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*92Ac9THHMIMTt8V0us-vqg.png\">\n",
    "\n",
    "예측 값과 실제 값의 차이인 오차, 그리고 그 오차들의 제곱합의 평균이 cost funtion이라고 가정해봅시다. \n",
    "우리의 목표는 이 cost function, 즉 오차를 최소화하는 w를 찾는 것입니다.\n",
    "위에서 가정한 cost function의 그래프는 단순한 2차함수이고, convex하기 때문에 쉽게 global minimum을 찾을 수 있습니다.\n",
    "\n",
    "cost function은 w에 대한 함수입니다. 제시된 cost function의 경우 단순화를 위해 w가 하나뿐이지만, 예측치에 여러 변수를 활용할 경우 변수 w는 여러 개로 늘어날 수 있습니다. \n",
    "\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*CAVpIlcKEBXPvZU7qz1xFw.png\">\n",
    "\n",
    "\n",
    "그렇다면 어떻게 오차를 최소화하는 w를 찾을 수 있을까요?\n",
    "시작지점은 무작위로 선택됩니다. (효율적인 최적화를 위해 시작지점을 컨트롤하는 방법도 있지만 여기서 이것까지 소개하지는 않겠습니다.)\n",
    "그리고 최솟값에 가까워지도록 이후 w를 조금씩 조정하는 것이 전부입니다.\n",
    "\n",
    "간단하죠?\n",
    "\n",
    "그렇다면 이제 w를 어떻게 minimum에 도달하도록 조정하는지만 알면 되겠군요.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*cdVe5TrBgR2ezgOGi46B_w.png\">\n",
    "\n",
    "\n",
    "Gradient descent는 한국어로 직역하면 '경사 하강'이라는 뜻입니다. \n",
    "우리는 이제 convex function의 기울기, 즉 경사를 타고 내려가 minimum에 접근할 생각입니다. \n",
    "그러기 위해서는 function의 기울기 값을 알아야하고, 그 값을 간단히 하기 위해 \\\\( \\frac{1}{2} \\\\) 을 곱해주었습니다.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1000/1*W9ArbeR20mIbemc7spRs8g.png\">\n",
    "\n",
    "\n",
    "위 식에서 := 라는 기호는 '할당'으로 이해하시면 됩니다.\n",
    "cost function을 미분한 gradient로 w 값을 갱신하겠다는 것입니다.\n",
    "그렇다면 저 \\\\( \\alpha \\\\), \\\\( \\alpha \\\\)가 도대체 무엇인지만 알면 이제 모든 것이 해결되겠군요.\n",
    "\n",
    "\\\\( \\alpha \\\\)는 위의 adaboostregressor에서 본 learning rate와 같은 개념입니다. 쉽게 말해 경사를 향해 내려가는 걸음걸이의 '보폭'이죠.\n",
    "보폭이 넓으면 그만큼 w값이 큰 차이로 갱신되겠지만, 섬세한 학습이 어렵습니다. Valley에서 진동하다가 minimum을 찾지 못하는 경우도 생긴다고 합니다. 반대로 보폭이 좁으면 w값이 드라마틱하게 갱신되지 않기 때문에 학습이 더디죠. 대신 좀 더 섬세하게 minimum을 찾아갈 겁니다. \n",
    "(위의 adaboostregressor에서 learning rate를 작게 설정하라고 한 이유가 여기에 있네요!)\n",
    "\n",
    "개인적으로는 학습속도를 \\\\( \\alpha \\\\)라는 단 하나의 파라미터로 컨트롤할 수 있어 정말 간편한 알고리즘이라고 생각합니다. \n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1000/1*UkIjo17ovvd4deEEt-KgWA.png\">\n",
    "\n",
    "gradient를 풀어써서 정리한 식입니다.\n",
    "**결과적으로, 기울기가 양의 값이면 w는 감소하는 방향으로 갱신되고,\n",
    "기울기가 음의 값이면 w는 증가하는 방향으로 갱신됩니다.**\n",
    "사실 이것만 이해(암기)하셔도 반절은 아는 것이나 다름없습니다. \n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*Al2r4PvGphnuqCJ3-YaARg.png\">\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*qv-mdHrWNPAysWVvhXCV9A.png\">\n",
    "\n",
    "두 가지 종류의 cost function을 비교한 식입니다. local minimum = global minimum이 아닌 경우에는 위 알고리즘이 제대로 동작하지 않으므로, 아래와 같은 convex fucntion을 loss fucntion으로 사용하라는 제언이었습다. 저런 식으로 convex하기만 하면 어떤 functino이라도 loss function으로 사용될 수 있다고 합니다! (그래서 가장 간단한 잔차제곱합을 예시로 드는 것이기도 하구요.)\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "여기까지 gradient descent에 대해 제가 찾아낸 가장 간단한 설명이었습니다.<br></br> \n",
    "그럼 이제 다시 gradient boosting으로 돌아와봅시다.<br></br>\n",
    "저 식을 boosting algorithm에 어떻게 적용하면 좋을까요?<br></br>\n",
    "\n",
    "핵심은 **Gradient Boosting이 loss function의 미분값(기울기 = gradient)를 다음 모델(Weak Learner)의 타겟으로 넘긴다**는 것입니다.<br></br> \n",
    "위처럼 Squared Error(잔차제곱합)를 쓰는 경우를 예로 들면, 현재 모델의 **잔차**를 타겟으로 놓고 다음 모델을 fitting시킨다는 것이죠.<br></br>\n",
    "그리고 기존 모델은 이 새로운 모델을 **흡수**해서 Bias, 즉 예측값의 오차를 줄이게 됩니다. <br></br>\n",
    "지속적으로 잔차를 구하고 모델을 피팅해서 더하기를 반복하는 것이죠.<br></br> \n",
    "매우 단순하고 직관적인 방법인데, 이를 loss funtion을 L2(잔차제곱합)로 설정한 Gradient Boosting으로 설명할 수 있습니다.<br></br>\n",
    "\n",
    "정리하자면 Gradient Boosting에서는 Gradient가 현재까지 학습된 모델의 약점을 드러내는 역할을 하고,<br></br> \n",
    "다른 모델이 그걸 중점적으로 보완하여 성능을 Boosting하게 됩니다. <br></br>\n",
    "위에서는 L2 손실함수를 썼지만 미분만 가능하다면 다른 손실함수도 얼마든지 쓸 수 있다는 것이 장점입니다. <br></br>\n",
    "(gradient descent를 설명하면서 마지막에 말했던 내용이죠?)<br></br>\n",
    "위 알고리즘의 특성상 계속 약점(오분류/잔차)을 보완하려고 하기 때문에, 잘못된 레이블링이나 아웃라이어에 필요 이상으로 민감할 수 있다고 합니다. <br></br>\n",
    "하지만 이런 문제에 강인한 L1 Loss나 Huber Loss를 쓰고자 한다면 그냥 손실함수만 교체하면 끝! <br></br>\n",
    "손실함수의 특성은 Gradient를 통해 자연스럽게 학습에 반영될 것입니다. (gradient가 손실함수의 편미분값이니까요!)<br></br>\n",
    "\n",
    "핵심을 수식 없이 요약하여 설명하다보니 다소 이해하기 어려운 내용이 되었을수도 있겠다는 생각이 드네요.<br></br>\n",
    "영어에 자신이 있으시다면 <a href=\"http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/\">이 kaggle master의 강의</a>를 추천합니다. <br></br>\n",
    "구체적인 예시로 누구나 gradient boosting을 쉽게 이해할 수 있도록 서술한 글입니다.<br></br>\n",
    "\n",
    " * L1 Loss나 Huber Loss 등의 용어에 대해 잘 모르신다면 <a href=\"https://ratsgo.github.io/machine%20learning/2017/10/12/terms/\">여기를 참고!\n",
    "</a><br></br>\n",
    " * Gradient가 residual보다 왜 좋은가에 대한 설명은 <a href=\"https://algorithmsdatascience.quora.com/GradientBoost-3-Why-gradient-not-residual\">여기</a>에 있습니다.<br></br>\n",
    "\n",
    "<img src=\"https://www.kdnuggets.com/wp-content/uploads/algorithm-iterations.jpg\" width=\"600px\">\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>GradientBoost practice</h2>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "그럼 실습시간입니다!<br></br>\n",
    "<br></br>\n",
    "\n",
    "데이터 전처리는 adaboost와 크게 다를 것이 없습니다.<br></br>\n",
    "역시나 이 또한 weak learner로 CART를 주로 사용하는 tree 기반 모델이기 때문이죠.<br></br>\n",
    "('주로' 라는 표현을 사용하긴 했지만 대부분 CART를 사용하고 있었습니다.)<br></br>\n",
    "\n",
    "Adaboost에서는 특이값을 조심하라고 했지만, 사실 그 부분은 GradientBoost에서도 어떤 형태의 loss function을 쓰느냐에 따라 달라지기 때문에 \n",
    "본인이 사용할 loss parameter를 생각하여 조정하면 될 문제라고 생각합니다.<br></br>\n",
    "<br></br>\n",
    "\n",
    "\n",
    "이제 중요한건 우리의 GradientBoost model이 오차를 지나치게 파고들어( = bias를 지나치게 줄여) overfitting되지 않도록 막는 것입니다.<br></br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "GradientBoostingRegressor(loss='ls', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, \n",
    "                          min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_split=1e-07, init=None, \n",
    "                          random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, presort='auto')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "파라미터를 조정하려고 하는데 adaboost에 비해 parameter가 꽤 많네요!<br></br>\n",
    "\n",
    "친절한 그림과 설명을 보도록 하겠습니다.<br></br>\n",
    "\n",
    "먼저 GBM의 parameter들은 크게 3개의 part로 분류할 수 있습니다.<br></br>\n",
    "\n",
    " - Tree-specific params. : 모델의 개별 트리(weak learner)에 영향\n",
    " - Boosting params. : Boosting 과정에 영향\n",
    " - 기타 :  다른 전체적인 기능 (또는 모델 성능에 별 영향을 끼치지 않음)\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "<img src=\"https://flonelin.files.wordpress.com/2016/07/untitled-picture1.png?w=1088\">\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "Tree-specific params.는 아래와 같습니다.<br></br>\n",
    "(Tree-specific param들은 RF를 포함한 대부분의 Tree ensemble 모델에서 조정하게 됩니다! 읽어보면 모두 당연한 내용들이라 명시하는 것 외에는 딱히 더 적을 내용이 없네요...)\n",
    "\n",
    " - Min_samples_split\n",
    " >  - 특정 기준으로 데이터를 분할시키기 위해 노드가 가져야할 최소한의 샘플 수\n",
    " >  - Over-fitting을 컨트롤하는데 사용됨. 높은 값은 트리에서 선택된 특정 샘플들에게 특이적으로 높을 수 있는 모델이 되는 것을 방지한다. (트리가 너무 깊게 파고들어가는 것을 방지!)\n",
    " >  - 오히려 너무 높은 값은 underfitting의 원인이 될 수 있기 때문에 Cross Validation을 사용하여 조정되어야한다.\n",
    "\n",
    " - Min_sample_leaf\n",
    " > - leaf 또는 말단 노드에서 요구되는 최소 샘플 수\n",
    " > - Min_samples_split 과 유사한 방식으로 Over-fitting을 컨트롤하는데 사용됨\n",
    " > - 일반적으로 낮은 값이 선택되어야 한다. (Imbalanced data의 경우 예측력이 높은 분류 기준이 최소 샘플 수 기준 때문에 사용되지 못할 수도 있습니다. 나뉘어진 한 쪽 노드의 데이터 갯수가 아주 적을 수도 있기 때문이지요.)\n",
    "\n",
    " - Min_weight_fraction_leaf\n",
    " > - Min_samples_leaf와 유사하지만 숫자대신에 샘플의 '비율'을 나타낸다.\n",
    " > - min_sample_leaf와 이것 중 하나만 정의되어야 한다. \n",
    " \n",
    " - Max_depth\n",
    " > - tree의 최대 깊이\n",
    " > - Overfitting을 조절하는데 사용되며, 깊이를 더 깊게 할수록 개별 샘플에 관계가 있는 모델이 된다.(개별 샘플에 관계있는 모델이라는 말은 bias가 낮은 모델을 말하는 것이겠죠?) Cross Validation을 사용하여 조정되어야한다.\n",
    "\n",
    " - Max_leaf_nodes\n",
    " > - 트리의 말단 노드 또는 잎사귀들의 최대 수\n",
    " > - Max_depth가 되는 위치를 정의할 수 있다.(n의 깊이가 최대 2^n개의 잎사귀를 생성한다는 말과 같으므로...) 정의된다면 max_depth를 무시한다.\n",
    "\n",
    " - Max_features\n",
    " > - 최선의 split을 찾기 위해 고려되어야하는 feature의 수. 정의하지 않을 경우 랜덤하게 선택된다.\n",
    " > - feature의 총 수의 root square가 일반적으로 좋은 결과를 보여주지만, 전체 feature 수의 30~40% 수준까지는 고려해보도록 한다.\n",
    " > - 높은 값일 수록 over-fitting을 유발하지만, 경우에 따라 다르다.\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br> \n",
    "\n",
    "Boosting params.는 아래와 같습니다.<br></br> \n",
    "\n",
    " - learning_rate\n",
    " > - adaboost에 있는 learning_rate와 같습니다. 복습을 위해 다시 한 번 언급하자면 '보폭'으로 기억해주세요. 짧은 보폭 + 많은 학습이 아무래도 견고한 모델을 만들겠죠? \n",
    "\n",
    " - n_estimators\n",
    " > - 잔차에 대해 자꾸 모델을 만들어내다보면 일정 수준 이상부터 overfit되겠죠. learning rate와 함께 적정 수준을 찾는 작업이 꼭 필요한 parameter입니다.\n",
    " \n",
    " - subsample\n",
    " > - 개별 모델(weak learner)들을 학습시킬 때 사용되는 데이터셋의 상대적 크기(비율)을 말합니다. 앞에서도 이는 resampling의 과정을 통해 만들어진다고 설명했습니다.(랜덤, 비복원추출) default는 1.0이지만 이보다 낮게 설정될 경우 Stochastic Gradient Boosting이 적용된다고 합니다. 보통 0.8 정도까지 낮출 때 결과가 좋지만 상황에 따라 다를 수 있다고 하네요. 1.0보다 낮게 설정하면 variance는 감소하고 bias는 증가하는 결과를 가져온다고 합니다.\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br> \n",
    "\n",
    "그리고 마지막으로 기타 잡다한 params.입니다.<br></br> \n",
    "\n",
    " - loss\n",
    " > -  adabooost에서 보았죠. 본인이 사용하고자 하는 loss function의 type을 고르는 parameter입니다. 일반적으로는 그냥 default인 최소제곱법을 사용해도 된다고 합니다.(ls = least square) 하지만 데이터에 따라 다를 수 있으니 huber나 linear한 loss funtion도 시도해보면 좋을 듯 합니다.\n",
    " \n",
    " - init\n",
    " > - Output의 초기화에 영향을 주는 parameter라고 합니다. 성능과는 큰 관계가 없는 것 같습니다. default값을 사용합시다.\n",
    " \n",
    " - random_state\n",
    " > - adaboost의 parameter에서 설명했습니다. 추가적으로 다른 parameter값을 사용한 모델간 성능을 객관적으로 비교하려면 seed를 고정하는 것이 좋다고 하네요. 그래야 정확히 같은 방식으로 resampling된 데이터를 사용할테니까요. grid search 과정에서 잊지않고 설정해주면 좋을듯해요.\n",
    " \n",
    " - verbose\n",
    " > - fitting 결과를 어떻게 출력할지를 결정하는 parameter로 성능과는 관계없습니다.\n",
    " \n",
    " - warm_start\n",
    " > - Model stacking과 흡사한 개념입니다. 즉, 이전에 fitting한 결과값을 타겟으로 하여 현재 모델을 fitting시키는 옵션입니다.시간이 많이 들고, 성능 개선도 미미하다고 하니 굳이 시도할 필요가 있을까 싶네요.\n",
    " \n",
    " - presort \n",
    " > - 개별 모델의 빠른 split을 위해 persort를 실행할지 여부를 결정하는 것입니다. 성능과는 큰 관련이 없어보입니다.  \n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br> \n",
    "\n",
    "그리고 결과적으로 overfitting을 피하기위한 일반적인 파라미터 튜닝 접근 순서는 이렇습니다.<br></br> \n",
    "\n",
    " 1. 처음에는 약간 높은 learning rate를 선택합니다. 일반적으로 0.1의 기본값을 쓰지만, 상황에 따라 0.05 ~ 0.2 사이에서 시작하는 것이 좋다고 합니다..\n",
    " 2. 설정한 learning rate에 적합한 트리 수를 찾습니다. 대략 40-70 정도를 잡는데요, 이렇게 적게 잡는 이유는 컴퓨팅 속도때문이라고 합니다. 트리 파라미터를 결정하기 위해 다양한 시나리오들을 테스트하려면 이터레이션 시간이 너무 길어져도 안되겠죠.\n",
    " 3. 이후 결정된 트리 수와 learning rate에 맞는 트리 파라미터를 튜닝해줍니다.\n",
    " 4. 더 낮은 learning rate와 더 높은 estimator를 사용하여 좀더 견고한 모델을 만듭니다.\n",
    " \n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br> \n",
    " \n",
    "음... 그리고 실습코드는 따로 준비하지 않았습니다. \n",
    "일단 sklearn에서 사용하는 method가 거의 다 비슷하구요, 노가다에 가까운 tuning을 굳이 보여드릴 필요도 없다고 생각되어서입니다.\n",
    "어느 데이터에나 쉽게 적용할 수 있는 boosting model이니 분석 프로젝트를 진행할 때 이 글을 참고하여 모델을 적용해보면 좋을 것 같습니다!\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>XGBoost</h2>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br> \n",
    "XGBoost는 앞에서 소개할 때에도 말씀드렸지만 GradientBoost를 효율적으로 개선한 모델이기 때문에 그 알고리즘의 작동 원리에 대해서는 더 설명드릴 내용이 없습니다. 따라서 XGBoost를 이해할 때 도움이 되는 참고자료만 제시하도록 하겠습니다.<br></br> \n",
    "\n",
    "parameter tuning도 GradientBoost와 아주 흡사하므로, 위의 내용을 이해하셨다면 XGBoost의 tuning 또한 어렵지않게 해내실 수 있을겁니다!<br></br>\n",
    "XGBoost는 컴퓨팅 측면에서 매우 개선된 가벼운 모델이라 기본적으로 estimator를 크게 두고 돌려도 시간이 그리 오래 걸리지 않습니다.<br></br> \n",
    "이 점 또한 감안하여 tuning한다면 좋은 모델을 얻을 수 있을 것입니다.<br></br>  \n",
    "<br></br>\n",
    "<br></br> \n",
    "\n",
    "**Reference**\n",
    "\n",
    " 1. XGBoost의 아버지인 Tianqi Chen의 Introduction입니다. 저는 수학을 잘 못해서 공부하다가 포기했는데요... 나중에 각잡고 다시 한번 도전해보아야겠습니다. 용래형 강의때 잘 들어둘걸 그랬네요... (http://delivery.acm.org/10.1145/2940000/2939785/p785-chen.pdf?ip=121.154.29.136&id=2939785&acc=CHORUS&key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E6D218144511F3437&__acm__=1518636854_73b3630df2c3f4007474d3af4021c50a)\n",
    " \n",
    " 2. 위 내용을 ppt로 정리한 것입니다. (https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)\n",
    " \n",
    " 3. XGBoost 구글링하면 가장 먼저 보게 되는 글. 매우 유익해요. (https://brunch.co.kr/@snobberys/137)\n",
    "\n",
    " 4. XGBoost 파이썬 패키지 (https://github.com/dmlc/xgboost/tree/master/python-package)\n",
    " \n",
    " 5. XGBoost 파이썬 패키지 Window에 설치하기 (https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/13043)\n",
    " \n",
    " 6. 무려 'Complete Guide'... 튜닝에 대한 자세한 설명이고 읽어보시면 GBM과 유사하다는 것을 확인할 수 있습니다. (https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n",
    "\n",
    " 7. 6을 번역한 내용. 별로 추천하고 싶진 않은데 시간이 급하면 이거라도 보시길! (https://flonelin.wordpress.com/2016/07/26/tuning-xgboostextream-gradient-boosting/)\n",
    " \n",
    " 8. 이것도 그냥 parameter 파악할 때 참고용으로만 쓰세요... (https://www.ibm.com/support/knowledgecenter/ko/SS3RA7_18.1.1/modeler_mainhelp_client_ddita/clementine/python_nodes_xgboost_tree_build.html)\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Reference</h2>\n",
    "\n",
    "http://xgboost.readthedocs.io/en/latest/model.html\n",
    "\n",
    "https://www.slideshare.net/freepsw/boosting-bagging-vs-boosting\n",
    "\n",
    "https://www.analyticsvidhya.com/?s=boosting\n",
    "https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "\n",
    "https://www.quora.com/\n",
    "https://www.quora.com/How-do-you-correct-for-overfitting-for-a-Gradient-Boosted-Machine\n",
    "\n",
    "https://datascienceschool.net/view-notebook/7d82087c31d64fe491dc74e1d5953ca2/\n",
    "\n",
    "https://m.blog.naver.com/jaehyubious/220375794266\n",
    "\n",
    "http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf\n",
    "\n",
    "http://scikit-learn.org/stable/index.html\n",
    "\n",
    "http://www.4four.us/article/2017/05/gradient-boosting-simply\n",
    "\n",
    "https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/\n",
    "\n",
    "https://medium.com/@peteryun/ml-%EB%AA%A8%EB%91%90%EB%A5%BC-%EC%9C%84%ED%95%9C-tensorflow-3-gradient-descent-algorithm-%EA%B8%B0%EB%B3%B8-c0688208fc59\n",
    "\n",
    "http://blog.naver.com/PostView.nhn?blogId=gksshdk8003&logNo=220911836024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
